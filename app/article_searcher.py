"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
"""

import requests
import time
from typing import List, Dict, Optional
from urllib.parse import quote, urlparse
import json
import re

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False


class ArticleSearcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_pubmed(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ PubMed
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "dental composite EMG")
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
        """
        articles = []
        
        try:
            # PubMed E-utilities API
            base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
            
            # –®–∞–≥ 1: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
            search_url = f"{base_url}esearch.fcgi"
            params = {
                'db': 'pubmed',
                'term': query,
                'retmax': max_results,
                'retmode': 'json'
            }
            
            response = self.session.get(search_url, params=params, timeout=10)
            if response.status_code != 200:
                return articles
            
            data = response.json()
            pmids = data.get('esearchresult', {}).get('idlist', [])
            
            if not pmids:
                return articles
            
            # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—å—è—Ö
            fetch_url = f"{base_url}efetch.fcgi"
            params = {
                'db': 'pubmed',
                'id': ','.join(pmids),
                'retmode': 'xml'
            }
            
            response = self.session.get(fetch_url, params=params, timeout=15)
            if response.status_code != 200:
                return articles
            
            # –ü–∞—Ä—Å–∏–Ω–≥ XML (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            xml_content = response.text
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ XML
            for pmid in pmids:
                article_info = self._parse_pubmed_xml(xml_content, pmid)
                if article_info:
                    articles.append(article_info)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–≤–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ API)
            time.sleep(0.5)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ PubMed: {e}")
        
        return articles
    
    def _parse_pubmed_xml(self, xml_content: str, pmid: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç XML –æ—Ç–≤–µ—Ç –æ—Ç PubMed –¥–ª—è –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é xml.etree.ElementTree)
            article_info = {
                'pmid': pmid,
                'url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                'source': 'PubMed'
            }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_match = re.search(r'<ArticleTitle[^>]*>(.*?)</ArticleTitle>', xml_content, re.DOTALL)
            if title_match:
                article_info['title'] = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤
            authors = []
            author_matches = re.findall(r'<Author[^>]*>.*?<LastName>(.*?)</LastName>.*?<ForeName>(.*?)</ForeName>', xml_content, re.DOTALL)
            for last_name, first_name in author_matches:
                authors.append(f"{first_name.strip()} {last_name.strip()}")
            if authors:
                article_info['authors'] = ', '.join(authors[:5])  # –ü–µ—Ä–≤—ã–µ 5 –∞–≤—Ç–æ—Ä–æ–≤
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞
            year_match = re.search(r'<PubDate>.*?<Year>(\d{4})</Year>', xml_content, re.DOTALL)
            if year_match:
                article_info['year'] = int(year_match.group(1))
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞
            journal_match = re.search(r'<Title>(.*?)</Title>', xml_content)
            if journal_match:
                article_info['journal'] = journal_match.group(1).strip()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–∞
            abstract_match = re.search(r'<AbstractText[^>]*>(.*?)</AbstractText>', xml_content, re.DOTALL)
            if abstract_match:
                article_info['abstract'] = re.sub(r'<[^>]+>', '', abstract_match.group(1)).strip()
                article_info['text'] = article_info['abstract']  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            
            return article_info if article_info.get('title') else None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ PubMed XML: {e}")
            return None
    
    def search_pubmed_central(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ PubMed Central (–æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø)
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
        """
        articles = []
        
        try:
            # PubMed Central API
            base_url = "https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi"
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —á–µ—Ä–µ–∑ PubMed, –∑–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤ PMC
            pubmed_articles = self.search_pubmed(query, max_results * 2)
            
            for article in pubmed_articles:
                pmid = article.get('pmid')
                if not pmid:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ PMC
                pmc_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmid}/"
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
                try:
                    response = self.session.get(pmc_url, timeout=10)
                    if response.status_code == 200 and 'pmc' in response.url.lower():
                        article['pmc_url'] = pmc_url
                        article['source'] = 'PubMed Central'
                        
                        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                        if BS4_AVAILABLE:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
                            main_content = soup.find('div', class_='tsec sec') or soup.find('div', id='maincontent')
                            if main_content:
                                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                                paragraphs = main_content.find_all('p')
                                full_text = '\n\n'.join([p.get_text() for p in paragraphs])
                                if full_text:
                                    article['text'] = full_text
                                    article['has_full_text'] = True
                except:
                    pass
                
                if article.get('has_full_text'):
                    articles.append(article)
                    if len(articles) >= max_results:
                        break
                
                time.sleep(0.3)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ PubMed Central: {e}")
        
        return articles
    
    def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –ø—Ä–µ–ø—Ä–∏–Ω—Ç–æ–≤ –Ω–∞ arXiv
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
        """
        articles = []
        
        if not FEEDPARSER_AVAILABLE:
            print("‚ö†Ô∏è feedparser –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install feedparser")
            return articles
        
        try:
            # arXiv API
            base_url = "http://export.arxiv.org/api/query"
            params = {
                'search_query': f'all:{query}',
                'start': 0,
                'max_results': max_results,
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }
            
            response = self.session.get(base_url, params=params, timeout=10)
            if response.status_code != 200:
                return articles
            
            feed = feedparser.parse(response.text)
            
            for entry in feed.entries:
                article_info = {
                    'title': entry.get('title', '').replace('\n', ' ').strip(),
                    'authors': ', '.join([author.get('name', '') for author in entry.get('authors', [])]),
                    'year': entry.get('published', '')[:4] if entry.get('published') else None,
                    'url': entry.get('link', ''),
                    'abstract': entry.get('summary', '').strip(),
                    'text': entry.get('summary', '').strip(),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    'source': 'arXiv',
                    'doi': entry.get('arxiv_doi', '')
                }
                
                if article_info['title']:
                    articles.append(article_info)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ arXiv: {e}")
        
        return articles
    
    def search_google_scholar_simple(self, query: str, max_results: int = 5) -> List[Dict]:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Google Scholar (–±–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ API)
        –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∞—Ç—å—è—Ö
        """
        articles = []
        
        if not BS4_AVAILABLE:
            print("‚ö†Ô∏è BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install beautifulsoup4")
            return articles
        
        try:
            # Google Scholar –ø–æ–∏—Å–∫
            search_url = "https://scholar.google.com/scholar"
            params = {
                'q': query,
                'hl': 'en'
            }
            
            response = self.session.get(search_url, params=params, timeout=10)
            if response.status_code != 200:
                return articles
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü–æ–∏—Å–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = soup.find_all('div', class_='gs_ri')[:max_results]
            
            for result in results:
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = result.find('h3', class_='gs_rt')
                if not title_elem:
                    continue
                
                title = title_elem.get_text().strip()
                
                # –°—Å—ã–ª–∫–∞
                link_elem = title_elem.find('a')
                url = link_elem.get('href', '') if link_elem else ''
                
                # –ê–≤—Ç–æ—Ä—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                authors_elem = result.find('div', class_='gs_a')
                authors_info = authors_elem.get_text().strip() if authors_elem else ''
                
                # –ê–±—Å—Ç—Ä–∞–∫—Ç
                abstract_elem = result.find('div', class_='gs_rs')
                abstract = abstract_elem.get_text().strip() if abstract_elem else ''
                
                article_info = {
                    'title': title,
                    'authors': authors_info.split(' - ')[0] if ' - ' in authors_info else authors_info,
                    'url': url if url.startswith('http') else f"https://scholar.google.com{url}",
                    'abstract': abstract,
                    'text': abstract,  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    'source': 'Google Scholar'
                }
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–≤—Ç–æ—Ä–∞—Ö
                year_match = re.search(r'\b(19|20)\d{2}\b', authors_info)
                if year_match:
                    article_info['year'] = int(year_match.group())
                
                if article_info['title']:
                    articles.append(article_info)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≤–µ–∂–ª–∏–≤–æ—Å—Ç–∏
            time.sleep(2)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Google Scholar: {e}")
            print("‚ö†Ô∏è Google Scholar –º–æ–∂–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
        
        return articles
    
    def search_all_sources(self, query: str, max_results_per_source: int = 5) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results_per_source: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        all_articles = []
        
        print(f"üîç –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'...")
        
        # PubMed
        print("üìö –ü–æ–∏—Å–∫ –≤ PubMed...")
        pubmed_articles = self.search_pubmed(query, max_results_per_source)
        all_articles.extend(pubmed_articles)
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(pubmed_articles)} —Å—Ç–∞—Ç–µ–π")
        
        # PubMed Central (–æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø)
        print("üìñ –ü–æ–∏—Å–∫ –≤ PubMed Central...")
        pmc_articles = self.search_pubmed_central(query, max_results_per_source)
        all_articles.extend(pmc_articles)
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(pmc_articles)} —Å—Ç–∞—Ç–µ–π")
        
        # arXiv (–ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã)
        print("üìÑ –ü–æ–∏—Å–∫ –≤ arXiv...")
        arxiv_articles = self.search_arxiv(query, max_results_per_source)
        all_articles.extend(arxiv_articles)
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(arxiv_articles)} —Å—Ç–∞—Ç–µ–π")
        
        # Google Scholar (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω)
        # print("üî¨ –ü–æ–∏—Å–∫ –≤ Google Scholar...")
        # scholar_articles = self.search_google_scholar_simple(query, max_results_per_source)
        # all_articles.extend(scholar_articles)
        # print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(scholar_articles)} —Å—Ç–∞—Ç–µ–π")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        seen_titles = set()
        unique_articles = []
        for article in all_articles:
            title = article.get('title', '').lower().strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        
        print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(unique_articles)}")
        
        return unique_articles
    
    def get_full_text_from_url(self, url: str) -> Optional[str]:
        """
        –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
        """
        if not BS4_AVAILABLE:
            return None
        
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ —Å—Ç–∏–ª–µ–π
            for script in soup(["script", "style"]):
                script.decompose()
            
            # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            main_content = (
                soup.find('article') or
                soup.find('div', class_='article-content') or
                soup.find('div', id='content') or
                soup.find('main') or
                soup.find('body')
            )
            
            if main_content:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                paragraphs = main_content.find_all('p')
                text = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                return text if text else None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å {url}: {e}")
        
        return None


def get_recommended_queries() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    return [
        "dental composite EMG",
        "composite material chewing teeth",
        "EMG masticatory muscles composite",
        "dental restoration composite selection",
        "polymerization shrinkage composite",
        "composite filler content wear resistance",
        "pathological tooth wear composite",
        "occlusion anomaly composite restoration",
        "bulk fill composite properties",
        "nanofilled composite mechanical properties"
    ]

