"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –º–æ–¥–µ–ª—å –∏ –æ–±—É—á–µ–Ω–∏–µ
–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
"""

import urllib.request
import urllib.parse
import json
import re
import time
import os
from pathlib import Path
from typing import List, Dict, Optional

# –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
MODEL_PATH = os.path.join(Path(__file__).parent.absolute(), "trained_model.pkl")
ARTICLES_CACHE_PATH = os.path.join(Path(__file__).parent.absolute(), "auto_loaded_articles.json")


def search_pubmed_simple(query: str, max_results: int = 5) -> List[Dict]:
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ PubMed —á–µ—Ä–µ–∑ E-utilities API
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python (urllib)
    """
    articles = []
    
    try:
        # –®–∞–≥ 1: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        search_url = f"{base_url}esearch.fcgi"
        
        params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json'
        }
        
        url_with_params = f"{search_url}?{urllib.parse.urlencode(params)}"
        
        with urllib.request.urlopen(url_with_params, timeout=10) as response:
            data = json.loads(response.read().decode('utf-8'))
            pmids = data.get('esearchresult', {}).get('idlist', [])
        
        if not pmids:
            return articles
        
        # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        fetch_url = f"{base_url}efetch.fcgi"
        params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml'
        }
        
        url_with_params = f"{fetch_url}?{urllib.parse.urlencode(params)}"
        
        with urllib.request.urlopen(url_with_params, timeout=15) as response:
            xml_content = response.read().decode('utf-8')
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ XML (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫)
        for pmid in pmids:
            article_info = parse_pubmed_xml_simple(xml_content, pmid)
            if article_info:
                articles.append(article_info)
        
        time.sleep(0.5)  # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ API
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ PubMed: {e}")
    
    return articles


def parse_pubmed_xml_simple(xml_content: str, pmid: str) -> Optional[Dict]:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ XML –æ—Ç–≤–µ—Ç–∞ –æ—Ç PubMed"""
    try:
        article_info = {
            'pmid': pmid,
            'url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
            'source': 'PubMed'
        }
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_match = re.search(r'<ArticleTitle[^>]*>(.*?)</ArticleTitle>', xml_content, re.DOTALL)
        if title_match:
            article_info['title'] = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤
        authors = []
        author_matches = re.findall(
            r'<Author[^>]*>.*?<LastName>(.*?)</LastName>.*?<ForeName>(.*?)</ForeName>',
            xml_content, re.DOTALL
        )
        for last_name, first_name in author_matches:
            authors.append(f"{first_name.strip()} {last_name.strip()}")
        if authors:
            article_info['authors'] = ', '.join(authors[:5])
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞
        year_match = re.search(r'<PubDate>.*?<Year>(\d{4})</Year>', xml_content, re.DOTALL)
        if year_match:
            article_info['year'] = int(year_match.group(1))
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞
        journal_match = re.search(r'<Title>(.*?)</Title>', xml_content)
        if journal_match:
            article_info['journal'] = journal_match.group(1).strip()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–∞
        abstract_match = re.search(r'<AbstractText[^>]*>(.*?)</AbstractText>', xml_content, re.DOTALL)
        if abstract_match:
            abstract = re.sub(r'<[^>]+>', '', abstract_match.group(1)).strip()
            article_info['abstract'] = abstract
            article_info['text'] = abstract  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        return article_info if article_info.get('title') else None
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ PubMed XML: {e}")
        return None


def search_arxiv_simple(query: str, max_results: int = 5) -> List[Dict]:
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø—Ä–µ–ø—Ä–∏–Ω—Ç–æ–≤ –Ω–∞ arXiv —á–µ—Ä–µ–∑ API
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python
    """
    articles = []
    
    try:
        base_url = "http://export.arxiv.org/api/query"
        params = {
            'search_query': f'all:{query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'relevance',
            'sortOrder': 'descending'
        }
        
        url_with_params = f"{base_url}?{urllib.parse.urlencode(params)}"
        
        with urllib.request.urlopen(url_with_params, timeout=10) as response:
            xml_content = response.read().decode('utf-8')
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ Atom feed
        entries = re.findall(r'<entry>(.*?)</entry>', xml_content, re.DOTALL)
        
        for entry in entries:
            title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
            title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else ''
            
            link_match = re.search(r'<id>(.*?)</id>', entry)
            url = link_match.group(1).strip() if link_match else ''
            
            summary_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
            abstract = re.sub(r'<[^>]+>', '', summary_match.group(1)).strip() if summary_match else ''
            
            authors = []
            author_matches = re.findall(r'<name>(.*?)</name>', entry)
            authors = ', '.join(author_matches[:5])
            
            published_match = re.search(r'<published>(\d{4})', entry)
            year = int(published_match.group(1)) if published_match else None
            
            if title:
                articles.append({
                    'title': title.replace('\n', ' ').strip(),
                    'authors': authors,
                    'year': year,
                    'url': url,
                    'abstract': abstract,
                    'text': abstract,
                    'source': 'arXiv'
                })
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ arXiv: {e}")
    
    return articles


def auto_load_and_train():
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π, –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –º–æ–¥–µ–ª—å –∏ –æ–±—É—á–µ–Ω–∏–µ
    """
    print("üîç –ù–∞—á–∏–Ω–∞—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π...")
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    queries = [
        "dental composite EMG masticatory muscles",
        "composite material chewing teeth restoration",
        "polymerization shrinkage composite occlusal",
        "composite filler content wear resistance",
        "pathological tooth wear composite restoration",
        "EMG masseter temporalis composite selection",
        "resin composite occlusal restoration EMG",
        "bulk fill composite posterior teeth",
        "composite restoration masticatory function",
        "EMG activity composite material selection",
        "dental composite mechanical properties EMG",
        "occlusal composite restoration EMG analysis"
    ]
    
    all_articles = []
    
    # –ü–æ–∏—Å–∫ –≤ PubMed (–±–æ–ª—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤, –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
    print("üìö –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ PubMed...")
    for query in queries[:8]:  # –ü–µ—Ä–≤—ã–µ 8 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ PubMed
        articles = search_pubmed_simple(query, max_results=5)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5
        all_articles.extend(articles)
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        time.sleep(0.8)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    
    # –ü–æ–∏—Å–∫ –≤ arXiv
    print("üìÑ –ü–æ–∏—Å–∫ –≤ arXiv...")
    for query in queries[8:]:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ –≤ arXiv
        articles = search_arxiv_simple(query, max_results=3)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2 –¥–æ 3
        all_articles.extend(articles)
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        time.sleep(0.8)
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    seen_titles = set()
    unique_articles = []
    for article in all_articles:
        title = article.get('title', '').lower().strip()
        if title and title not in seen_titles:
            seen_titles.add(title)
            unique_articles.append(article)
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(unique_articles)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ –∫—ç—à
    try:
        with open(ARTICLES_CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(unique_articles, f, ensure_ascii=False, indent=2)
        print(f"üíæ –°—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {ARTICLES_CACHE_PATH}")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π: {e}")
    
    return unique_articles


def train_model_with_articles(articles: List[Dict]):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö –∏ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        from knowledge_extractor import KnowledgeExtractor
        from model_trainer import ClinicalDataExtractor, CompositeModelTrainer, EMGCompositePair
        
        print("\nü§ñ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∑–Ω–∞–Ω–∏–π
        knowledge_extractor = KnowledgeExtractor()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ knowledge_extractor
        for article_data in articles:
            try:
                article = knowledge_extractor.add_article(
                    title=article_data.get('title', ''),
                    text=article_data.get('text', article_data.get('abstract', '')),
                    url=article_data.get('url', ''),
                    authors=article_data.get('authors', ''),
                    year=article_data.get('year'),
                    journal=article_data.get('journal', '')
                )
                knowledge_extractor.process_article(article)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ '{article_data.get('title', 'Unknown')}': {e}")
                continue
        
        print(f"üìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(knowledge_extractor.articles)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏
        try:
            from clinical_articles_data import get_clinical_articles, get_emg_composite_pairs
            clinical_articles = get_clinical_articles()
            clinical_pairs_dicts = get_emg_composite_pairs()
            
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π: {len(clinical_articles)}")
            print(f"üìä –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ –ø–∞—Ä –≠–ú–ì-–∫–æ–º–ø–æ–∑–∏—Ç (—Å–ª–æ–≤–∞—Ä–∏): {len(clinical_pairs_dicts)}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏ –≤ knowledge_extractor
            for article_data in clinical_articles:
                try:
                    article = knowledge_extractor.add_article(**article_data)
                    knowledge_extractor.process_article(article)
                except Exception as e:
                    continue
        except ImportError:
            clinical_pairs_dicts = []
            print("‚ö†Ô∏è –ú–æ–¥—É–ª—å clinical_articles_data –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        extractor = ClinicalDataExtractor()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤ –æ–±—ä–µ–∫—Ç—ã EMGCompositePair –∏ –¥–æ–±–∞–≤–ª—è–µ–º
        base_pairs = []
        if clinical_pairs_dicts:
            for pair_dict in clinical_pairs_dicts:
                try:
                    pair = EMGCompositePair(**pair_dict)
                    base_pairs.append(pair)
                    extractor.extracted_pairs.append(pair)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –ø–∞—Ä—ã: {e}")
                    continue
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(extractor.extracted_pairs)} –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        try:
            from generate_synthetic_training_data import get_all_synthetic_pairs
            print("\nüî¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
            synthetic_pairs = get_all_synthetic_pairs(base_pairs)
            extractor.extracted_pairs.extend(synthetic_pairs)
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(synthetic_pairs)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä")
        except ImportError as e:
            print(f"‚ö†Ô∏è –ú–æ–¥—É–ª—å generate_synthetic_training_data –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            traceback.print_exc()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        for article_data in articles:
            text = article_data.get('text', article_data.get('abstract', ''))
            if text:
                try:
                    extractor.extract_patient_data(
                        text,
                        article_title=article_data.get('title', ''),
                        article_url=article_data.get('url', ''),
                        article_year=article_data.get('year')
                    )
                except Exception as e:
                    continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π
        for article_data in clinical_articles:
            text = article_data.get('text', '')
            if text:
                try:
                    extractor.extract_patient_data(
                        text,
                        article_title=article_data.get('title', ''),
                        article_url=article_data.get('url', ''),
                        article_year=article_data.get('year')
                    )
                except Exception as e:
                    continue
        
        print(f"üìä –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä –≠–ú–ì-–∫–æ–º–ø–æ–∑–∏—Ç: {len(extractor.extracted_pairs)}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if len(extractor.extracted_pairs) > 0:
            trainer = CompositeModelTrainer()
            
            print("üéì –û–±—É—á–∞—é –º–æ–¥–µ–ª—å —Å –∞–Ω—Å–∞–º–±–ª–µ–º –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏...")
            try:
                results = trainer.train(extractor.extracted_pairs, model_type='random_forest', use_ensemble=True)
                
                if trainer.model is not None:
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    trainer.save_model(MODEL_PATH)
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")
                    if results.get('accuracy'):
                        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {results['accuracy']:.2%}")
                    print(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {results.get('train_size', 0)}")
                    print(f"üî¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ç–æ–≤: {results.get('unique_composites', 0)}")
                    return True
                else:
                    print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")
                    return False
            except ValueError as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
                return False
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –≠–ú–ì-–∫–æ–º–ø–æ–∑–∏—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
            
    except ImportError as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π: {e}")
        import traceback
        traceback.print_exc()
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return False


def auto_train_on_startup():
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∏ –µ—Å–ª–∏ –Ω–µ—Ç - –æ–±—É—á–∞–µ—Ç
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    if os.path.exists(MODEL_PATH):
        print(f"‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {MODEL_PATH}")
        return True
    
    print("üîÑ –û–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ù–∞—á–∏–Ω–∞—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Å—Ç–∞—Ç–µ–π
    articles = []
    if os.path.exists(ARTICLES_CACHE_PATH):
        try:
            with open(ARTICLES_CACHE_PATH, 'r', encoding='utf-8') as f:
                articles = json.load(f)
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ –∫—ç—à–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞ —Å—Ç–∞—Ç–µ–π: {e}")
    
    # –ï—Å–ª–∏ —Å—Ç–∞—Ç–µ–π –Ω–µ—Ç, –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
    if not articles:
        articles = auto_load_and_train()
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    if articles:
        success = train_model_with_articles(articles)
        return success
    else:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return False


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    auto_train_on_startup()

